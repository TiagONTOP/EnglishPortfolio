{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f8c09a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is dedicated to building a regression model based on LGBMRegressor (Light Gradient Boosting Machine) to predict the sales of a set of stores. Various data sources are considered for this purpose:\n",
    "\n",
    "1. General information regarding sales and stores (files `train.csv`, `test.csv`, and `stores.csv`),\n",
    "2. Variations in oil prices (`oil.csv`), a factor that can influence consumer purchasing power,\n",
    "3. Public holidays and special events (`holidays_events.csv`) that may affect buying behaviors,\n",
    "4. Transaction records (`transactions.csv`).\n",
    "\n",
    "After importing this data, the notebook proceeds with a series of preprocessing steps, including merging different sources of information, filling in missing values, and encoding categorical data. Additionally, additional time-based features are added to enrich the model.\n",
    "\n",
    "Once these preparation steps are completed, the data is split into a training set and a test set, and an LGBMRegressor model is trained on this data. Predictions are then generated and evaluated using the mean squared logarithmic error. Finally, negative predictions are adjusted to zero since a sales prediction cannot be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d25e4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1966285",
   "metadata": {},
   "source": [
    "These lines of code are used to import multiple datasets from CSV files and prepare the data for further analysis.\n",
    "\n",
    "1. `holidays = pd.read_csv('holidays_events.csv', parse_dates=True)`: Imports the special events and holidays data, ensuring that all dates are correctly parsed as datetime objects.\n",
    "\n",
    "2. `oil = pd.read_csv('oil.csv', parse_dates=True, index_col=0)`: Imports the oil price data, while setting the first column (which is the date) as the DataFrame's index.\n",
    "\n",
    "3. `sample_submission = pd.read_csv('sample_submission.csv')`: Imports the submission template used to properly format our predictions before submitting them for evaluation.\n",
    "\n",
    "4. `stores = pd.read_csv('stores.csv')`: Imports detailed information about each store.\n",
    "\n",
    "5. `data_test = pd.read_csv('test.csv')` and `data_train = pd.read_csv('train.csv')`: Import the test and training data respectively. These datasets include detailed information about each transaction.\n",
    "\n",
    "6. `transactions = pd.read_csv('transactions.csv')`: Imports information about all transactions made.\n",
    "\n",
    "7. `data_test['date'] = pd.to_datetime(data_test.date)`, `data_train['date'] = pd.to_datetime(data_train.date)`, and `holidays['date'] = pd.to_datetime(holidays.date)`: Converts the date column in the test, training, and holidays DataFrames to datetime. This makes it easier to further process dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fc90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.read_csv('holidays_events.csv', parse_dates=True)\n",
    "oil = pd.read_csv('oil.csv', parse_dates=True, index_col=0)\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "data_train = pd.read_csv('train.csv')\n",
    "transactions = pd.read_csv('transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b1768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['date'] = pd.to_datetime(data_test.date)\n",
    "data_train['date'] = pd.to_datetime(data_train.date)\n",
    "holidays['date'] = pd.to_datetime(holidays.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9235ec8",
   "metadata": {},
   "source": [
    "This portion of code performs several preprocessing operations on the data:\n",
    "\n",
    "1. **Creation of the 'oil' index:** It starts by creating a new index for the 'oil' DataFrame. This index is a combination of unique dates present in both the 'data_train' and 'data_test' DataFrames. The missing values in the 'oil' index are then filled using the 'forward fill' and 'backward fill' methods.\n",
    "\n",
    "2. **DataFrames merging:** The 'data_train' and 'data_test' DataFrames are then enriched by merging them with the 'oil', 'stores', and 'holidays' DataFrames on the appropriate columns. The 'left' method is used, which means that all rows from the 'data_train' and 'data_test' DataFrames are preserved, and only the corresponding rows from the other DataFrames are used.\n",
    "\n",
    "3. **Handling missing values:** Next, all columns with missing values in 'data_test' are identified. These missing values are filled with the value 'None' in both 'data_train' and 'data_test'.\n",
    "\n",
    "4. **Data factorization:** The 'data_train' and 'data_test' DataFrames are then concatenated into a single DataFrame called 'data_factorize', and all columns of type 'object' are factorized. This means that unique string values are replaced with integer numbers, which is often necessary for machine learning algorithms.\n",
    "\n",
    "5. **Train-test data separation:** The 'data_factorize' DataFrame is then split back into 'data_train' and 'data_test' using the previously stored index ('test_idx').\n",
    "\n",
    "6. **Adding time features:** Finally, the 'add_time_cols' function is defined and used to add additional time features ('day', 'month', 'year') to the 'data_train' and 'data_test' DataFrames. The 'date' column is then removed from these DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cadb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_oil = pd.Index((pd.concat([pd.Series(data_train.date.unique()), pd.Series(data_test.date.unique())], axis=0)\n",
    "                     .reset_index()\n",
    "                     .drop(['index'], axis=1))[0])\n",
    "index_oil.name = 'date'\n",
    "oil = oil.reindex(index_oil).reset_index()\n",
    "oil = oil.fillna(method='ffill').fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9594e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.merge(data_train, oil.reset_index(), on='date', how='left').drop(['index'], axis=1)\n",
    "data_test = pd.merge(data_test, oil.reset_index(), on='date', how='left').drop(['index'], axis=1)\n",
    "data_train = pd.merge(data_train, stores, on='store_nbr', how='left')\n",
    "data_test = pd.merge(data_test, stores, on='store_nbr', how='left')\n",
    "data_train = pd.merge(data_train, holidays.drop_duplicates(subset='date'), on='date', how='left')\n",
    "data_test = pd.merge(data_test, holidays.drop_duplicates(subset='date'), on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3e90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.set_index('id')\n",
    "data_test = data_test.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed636b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_col = [c for c in data_test.isna().any()[data_test.isna().any()].index]\n",
    "data_train[na_col] = data_train[na_col].fillna(value='None')\n",
    "data_test[na_col] = data_test[na_col].fillna(value='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "982bcbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = data_test.index[0]\n",
    "data_factorize = pd.concat([data_train, data_test], axis=0)\n",
    "for col, types in zip(data_factorize, data_factorize.dtypes):\n",
    "    if types == 'object':\n",
    "        data_factorize[col] = pd.factorize(data_factorize[col])[0]\n",
    "        \n",
    "data_test = data_factorize.iloc[test_idx:].drop(['sales'], axis=1)\n",
    "data_train = data_factorize.iloc[:test_idx]\n",
    "del data_factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dfaaf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_cols(data):\n",
    "    data['day'] = data['date'].dt.dayofweek\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data.drop(['date'], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "data_train, data_test = add_time_cols(data_train), add_time_cols(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2c7ce",
   "metadata": {},
   "source": [
    "This portion of code corresponds to training and evaluating the regression model:\n",
    "\n",
    "1. **Separation of features and target:** The training data is divided into features (X) and target (y). The target is the 'sales' column that the model will try to predict.\n",
    "\n",
    "2. **Splitting the data into training and test sets:** The data is then split into a training set and a test set using the `train_test_split` function from scikit-learn. 80% of the data is used for training the model, and the remaining 20% is used to test its performance. The `shuffle=False` option ensures that the data is not shuffled before the split, which is important for preserving the temporal structure of the data.\n",
    "\n",
    "3. **Training the model:** A LightGBM regression model is then trained on the training set. The `n_jobs=-1` option allows using all the CPU cores for training.\n",
    "\n",
    "4. **Prediction and evaluation:** The model is used to predict sales on the test set. Negative predictions are adjusted to 0 because a sales prediction cannot be negative. Finally, the mean squared logarithmic error (mean squared log error) between the predicted values and the actual values is calculated and displayed.\n",
    "\n",
    "5. **Checking predictions:** Lastly, the minimum value of the predictions is calculated and displayed. This is probably done to verify that negative predictions have been adjusted to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96cfb3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_train.drop(['sales'], axis=1), data_train['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ddaeca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79349fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.118522077604036\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "preds = np.where(preds < 0, 0, preds)\n",
    "print(mean_squared_log_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a73be451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125.5684359719774"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(preds < 0, 0, preds).min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
